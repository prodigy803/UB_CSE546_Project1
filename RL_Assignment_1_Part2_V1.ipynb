{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class environment_q_learning:\n",
    "    def __init__(self,type_of_env:str,gamma_disc: float, epsilon:float,epsilon_decay:float,learning_rate:float,no_of_episodes:int,max_time_steps:int):\n",
    "\n",
    "        if max_time_steps < 10:\n",
    "            raise ValueError(\"Timesteps should be greater than of equal to 10\")\n",
    "        \n",
    "        if (epsilon_decay > 1) or (epsilon_decay < 0):\n",
    "            raise ValueError(\"Epsilon decay should be less than 1 and greater than 0\")\n",
    "        \n",
    "        if no_of_episodes < 1:\n",
    "            raise ValueError(\"No of Episodes should be atleast equal to 1\")\n",
    "\n",
    "        # No of number of states = 25 - Requirement 1\n",
    "        self.environment = np.zeros((5,5))\n",
    "\n",
    "        # No of actions an agent can take:\n",
    "        self.action_set_size = 4\n",
    "\n",
    "        self.gamma = gamma_disc\n",
    "        # Q Value Learning Table\n",
    "        # self. = np.zeros((len(self.environment.flatten())),self.action_set_size)\n",
    "        self.qvalue_table = {}\n",
    "        for i1 in range(5):\n",
    "            for i2 in range(5):\n",
    "                for i in np.zeros((25,4)):\n",
    "                    self.qvalue_table[(i1,i2)] = i\n",
    "        \n",
    "        self.max_timesteps = max_time_steps\n",
    "        self.current_time_steps = 0\n",
    "        \n",
    "        # This determines the exploitation vs exploration phase.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # this determines the reduction in epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # this determines how quickly the q values for a state are updated\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # this tells us the no_of_epsiodes during which we will determine the optimal q value\n",
    "        self.no_of_episodes = no_of_episodes\n",
    "        \n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "\n",
    "        self.environment[tuple(self.agent_current_pos)] = 1\n",
    "        self.environment[tuple(self.goal_pos)] = 0.5\n",
    "\n",
    "        # Collection of Rewards (the keys) and associated values (the states). -> Total No of Rewards = 4 -> Requirement 3\n",
    "        self.rewards = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]}]\n",
    "        self.reward_states = [(0,3),(3,0),(2,3),(3,2)]\n",
    "        # Setting the colors for the reward states in the environment.\n",
    "        for reward_state in self.rewards:\n",
    "            for reward, position in reward_state.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        # Either Deterministic or stochastic.\n",
    "        self.environment_type = type_of_env\n",
    "\n",
    "        # This tracks the reward for the agent.\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Here we are essentially resetting all the values.\n",
    "        self.current_time_steps = 0\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "        \n",
    "        self.environment = np.zeros((5,5))\n",
    "        \n",
    "        self.rewards = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]},{10:[4,4]}]\n",
    "        self.reward_states = [(0,3),(3,0),(2,3),(3,2),[4,4]]\n",
    "        \n",
    "        for reward in self.rewards:\n",
    "            for reward, position in reward.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        self.environment[self.agent_current_pos] = 1\n",
    "        self.environment[self.goal_pos] = 0.5\n",
    "    \n",
    "    def step(self):\n",
    "        print(\"Steps starting agent pos is\", self.agent_current_pos)\n",
    "        # We are checking wether the environment is deterministic or stochastic\n",
    "        if self.environment_type == 'deterministic':\n",
    "            # In Deterministic environments, there is no use for epsilon as all the actions are deterministic / greedy / pre-determined.\n",
    "            \n",
    "            self.epsilon = 0\n",
    "            self.current_time_steps +=1\n",
    "\n",
    "            # get_all_possible_actions\n",
    "            all_possible_actions = self.get_all_possible_actions(self.agent_current_pos)\n",
    "            # all_possible_actions -> contains a list of actions that a agent can take from the state its in\n",
    "            \n",
    "            states_the_actions_lead_to = self.get_states_for_actions(all_possible_actions,self.agent_current_pos)\n",
    "            \n",
    "            current_max = None\n",
    "\n",
    "            # print('IN the current states, here are the states that can be reached, ', states_the_actions_lead_to)\n",
    "            \n",
    "            current_max_state = states_the_actions_lead_to[0]\n",
    "            current_max_states = []\n",
    "            \n",
    "            # all_possible_actions -> [1,3] like that a single list\n",
    "            # states_the_actions_lead_to -> [[1,0],[0,1]] -> states which the all_possible_actions lead to\n",
    "            for state_result, action in zip(states_the_actions_lead_to,all_possible_actions):\n",
    "                \n",
    "                # here we are fetching the max q value for the states that we can get into\n",
    "\n",
    "                max_q_value_of_state_result = max(self.qvalue_table[tuple([state_result[0],state_result[1]])])\n",
    "                \n",
    "                reward = self.check_and_get_reward(state_result)\n",
    "                \n",
    "                q_value = reward + self.gamma * max_q_value_of_state_result\n",
    "                \n",
    "                print('Updated Q Table for agent at ',self.agent_current_pos, ' for action ', action,' with  q value',q_value)\n",
    "                self.qvalue_table[tuple([self.agent_current_pos[0],self.agent_current_pos[1]])][action] = q_value\n",
    "                \n",
    "                if current_max == None:\n",
    "                    current_max_state = state_result\n",
    "                    current_max = q_value\n",
    "                    current_max_states = []\n",
    "                    \n",
    "                if current_max < q_value:\n",
    "                    current_max_state = state_result\n",
    "                    current_max = q_value\n",
    "                    current_max_states = []\n",
    "\n",
    "                elif current_max == q_value:\n",
    "                    current_max_states.append(state_result)\n",
    "            \n",
    "            if len(current_max_states) != 0:        \n",
    "                print('equal states is ,', current_max_states)        \n",
    "                self.agent_current_pos = random.choice(current_max_states)\n",
    "                # print('changing the value of agent current pos to, ',self.agent_current_pos)\n",
    "            else:\n",
    "                # print('changing the value of agent current pos to, ',current_max_state)\n",
    "                # print(current_max_states)\n",
    "                self.agent_current_pos = current_max_state\n",
    "\n",
    "            # # Here we are clipping the agents position to be in the environment (i.e if the agent goes out of env, we shall clip him to be inside the environment).\n",
    "            # self.agent_current_pos = list(np.clip(self.agent_current_pos, 0, 4))\n",
    " \n",
    "            # Here we are calculating the reward (i.e. the cumulative reward) and deleting that reward state from the collection of the reward states.\n",
    "            breaker = False\n",
    "            for reward_state_counter in range(len(self.rewards)):\n",
    "                for reward, state in self.rewards[reward_state_counter].items():\n",
    "                    # if the reward state matches the agents, sum the cum reward and delete that particular reward state space.\n",
    "\n",
    "                    if state == self.agent_current_pos:\n",
    "                        self.cumulative_reward += reward\n",
    "                        del self.rewards[reward_state_counter]\n",
    "                        breaker = True\n",
    "                        break\n",
    "\n",
    "                if breaker:\n",
    "                    break\n",
    "            \n",
    "            # We are now re-visualizing the environment\n",
    "            self.environment = np.zeros((5,5)) \n",
    "\n",
    "            for reward_state_counter in range(len(self.rewards)):\n",
    "                for reward, position in self.rewards[reward_state_counter].items():\n",
    "                    self.environment[tuple(position)] = reward\n",
    "                \n",
    "            self.environment[tuple(self.goal_pos)] = 0.5\n",
    "            self.environment[tuple(self.agent_current_pos)] = 1\n",
    "            \n",
    "            # if the agent has reached the final state then done\n",
    "            if (self.agent_current_pos == self.goal_pos) or (self.current_time_steps == self.max_timesteps):\n",
    "                done_or_not = True\n",
    "            \n",
    "            else:\n",
    "                done_or_not = False\n",
    "\n",
    "            return self.environment.flatten, self.cumulative_reward, done_or_not, self.current_time_steps\n",
    "    \n",
    "    def get_state_based_on_action(self, action, state):\n",
    "        state_copy = state.copy()\n",
    "        if action == 0:\n",
    "            # print('Up')\n",
    "            state_copy[0] -=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 1:\n",
    "            # print('Down')\n",
    "            state_copy[0] +=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 2:\n",
    "            # print('Our Left or the Agents Right')\n",
    "            state_copy[1] -=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 3:\n",
    "            # print('Our Right or the Agents Left')\n",
    "            state_copy[1] +=1\n",
    "            return state_copy\n",
    "\n",
    "    def check_and_get_reward(self, state_result):\n",
    "        # print('checking rewards for, ', state_result)\n",
    "        for i in range(len(self.rewards)):\n",
    "            for key, value in self.rewards[i].items():\n",
    "                if value == state_result:\n",
    "                    # self.rewards.pop(i)\n",
    "                    # print('found reward for state_result', state_result, value)\n",
    "                    return key\n",
    "\n",
    "        return 0\n",
    "\n",
    "    # def get_all_q_values_for_states(self,all_actions,agent_state):\n",
    "    def get_states_for_actions(self, all_actions, agent_state):\n",
    "    \n",
    "        temp_states = agent_state.copy() \n",
    "        # vals_to_be_returned = []\n",
    "        states_considered = []\n",
    "\n",
    "        for action_to_be_taken in all_actions:\n",
    "\n",
    "            action_taken = self.get_state_based_on_action(action_to_be_taken,agent_state)\n",
    "            \n",
    "            states_considered.append(action_taken)\n",
    "\n",
    "            agent_state = temp_states.copy()\n",
    "\n",
    "        return states_considered\n",
    "\n",
    "    def get_all_possible_actions(self,agent_current_pos):\n",
    "        \n",
    "        x_pos = agent_current_pos[0]\n",
    "        y_pos = agent_current_pos[1]\n",
    "\n",
    "        if (x_pos == 0) and (y_pos == 0):\n",
    "            return [1,3]\n",
    "        \n",
    "        elif (x_pos == 4) and (y_pos == 0):\n",
    "            return [0,3]\n",
    "        \n",
    "        elif (x_pos == 0) and (y_pos == 4):\n",
    "            return [1,2]\n",
    "        \n",
    "        elif (x_pos == 0) and (y_pos <= 3):\n",
    "            return [1,2,3]\n",
    "        \n",
    "        elif (x_pos <= 3) and (y_pos == 0 ):\n",
    "            return [0,1,3]\n",
    "        \n",
    "        elif (x_pos == 4) and (y_pos <= 3 ):\n",
    "            return [0,2,3]\n",
    "\n",
    "        elif (x_pos < 4) and (y_pos == 4 ):\n",
    "            return [0,2,1]\n",
    "\n",
    "        elif (x_pos >=1) and (x_pos<4) and (y_pos >= 1) and (y_pos <4):\n",
    "            return [0,1,2,3]\n",
    "\n",
    "        elif (x_pos==4) and (y_pos == 4):\n",
    "            return 'Yay youve won'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def render(self):\n",
    "        plt.imshow(self.environment)\n",
    "        plt.show()\n",
    "\n",
    "    def train(self):\n",
    "        # done = False\n",
    "        self.current_time_steps =0\n",
    "        for i in range(self.no_of_episodes):\n",
    "            print(i)\n",
    "            done = False\n",
    "            while not done:\n",
    "                # self.render()\n",
    "                observation, reward, done, _ = self.step()\n",
    "                if done:\n",
    "                    print(reward)\n",
    "            self.reset()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sudo code for \n",
    "## Deterministic Q-Learning Value\n",
    "1. Get all possible actions when ia particular state:\n",
    "2. Calculate the Q-values for transistioning into the possible states (based on the actions available in step 1)\n",
    "    1. Search the Q-table for the max q-values for the different states\n",
    "        - If all max of all the transistion states is the same, select a random action.\n",
    "    2. If there is a reward for that state, take that into consideration.\n",
    "    3. Calculate the Q-values for all the actions for the state that the agent is in.\n",
    "    4. Update the Q-value table for the different actions by applying the learning rate\n",
    "    4. update the agent state based on the max q value in step 5.\n",
    "    5. Go to step main 2\n",
    "3. After we reach the goal state, reduce the gamma and the learning rate and restart the episode while keepng the epochs the same."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# class environment_q_learning:\n",
    "#     def __init__(self,type_of_env:str,gamma_disc: float, epsilon:float,epsilon_decay:float,learning_rate:float,no_of_episodes:int,max_time_steps:int):\n",
    "environment_q_learning_obj = environment_q_learning(type_of_env = 'deterministic',gamma_disc=0.9,epsilon=0.2,epsilon_decay=0.001,learning_rate=0.1,no_of_episodes=1,max_time_steps=50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "environment_q_learning_obj.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Steps starting agent pos is [0, 0]\n",
      "Updated Q Table for agent at  [0, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 0], [0, 1]]\n",
      "Steps starting agent pos is [0, 1]\n",
      "Updated Q Table for agent at  [0, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 1], [0, 0], [0, 2]]\n",
      "Steps starting agent pos is [0, 2]\n",
      "Updated Q Table for agent at  [0, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 2]  for action  3  with  q value -1.0\n",
      "equal states is , [[1, 2], [0, 1]]\n",
      "Steps starting agent pos is [0, 1]\n",
      "Updated Q Table for agent at  [0, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 1], [0, 0], [0, 2]]\n",
      "Steps starting agent pos is [0, 0]\n",
      "Updated Q Table for agent at  [0, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 0], [0, 1]]\n",
      "Steps starting agent pos is [0, 1]\n",
      "Updated Q Table for agent at  [0, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 1], [0, 0], [0, 2]]\n",
      "Steps starting agent pos is [0, 2]\n",
      "Updated Q Table for agent at  [0, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 2]  for action  3  with  q value -1.0\n",
      "equal states is , [[1, 2], [0, 1]]\n",
      "Steps starting agent pos is [0, 1]\n",
      "Updated Q Table for agent at  [0, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 1], [0, 0], [0, 2]]\n",
      "Steps starting agent pos is [0, 0]\n",
      "Updated Q Table for agent at  [0, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 0], [0, 1]]\n",
      "Steps starting agent pos is [1, 0]\n",
      "Updated Q Table for agent at  [1, 0]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 0], [2, 0], [1, 1]]\n",
      "Steps starting agent pos is [1, 1]\n",
      "Updated Q Table for agent at  [1, 1]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Steps starting agent pos is [0, 1]\n",
      "Updated Q Table for agent at  [0, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 1], [0, 0], [0, 2]]\n",
      "Steps starting agent pos is [0, 0]\n",
      "Updated Q Table for agent at  [0, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 0], [0, 1]]\n",
      "Steps starting agent pos is [1, 0]\n",
      "Updated Q Table for agent at  [1, 0]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 0], [2, 0], [1, 1]]\n",
      "Steps starting agent pos is [0, 0]\n",
      "Updated Q Table for agent at  [0, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [0, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 0], [0, 1]]\n",
      "Steps starting agent pos is [1, 0]\n",
      "Updated Q Table for agent at  [1, 0]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 0]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 0], [2, 0], [1, 1]]\n",
      "Steps starting agent pos is [2, 0]\n",
      "Updated Q Table for agent at  [2, 0]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 0]  for action  1  with  q value -1.0\n",
      "Updated Q Table for agent at  [2, 0]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 0], [2, 1]]\n",
      "Steps starting agent pos is [2, 1]\n",
      "Updated Q Table for agent at  [2, 1]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[1, 1], [3, 1], [2, 0], [2, 2]]\n",
      "Steps starting agent pos is [1, 1]\n",
      "Updated Q Table for agent at  [1, 1]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Steps starting agent pos is [1, 2]\n",
      "Updated Q Table for agent at  [1, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Steps starting agent pos is [1, 1]\n",
      "Updated Q Table for agent at  [1, 1]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 1]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Steps starting agent pos is [1, 2]\n",
      "Updated Q Table for agent at  [1, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [1, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 3.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 3.0\n",
      "equal states is , [[2, 3]]\n",
      "Steps starting agent pos is [2, 3]\n",
      "Updated Q Table for agent at  [2, 3]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  2  with  q value 2.7\n",
      "Updated Q Table for agent at  [2, 3]  for action  3  with  q value 0.0\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 3.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 2.43\n",
      "Steps starting agent pos is [3, 2]\n",
      "Updated Q Table for agent at  [3, 2]  for action  0  with  q value 2.7\n",
      "Updated Q Table for agent at  [3, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[2, 2]]\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 2.43\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 2.43\n",
      "equal states is , [[2, 3]]\n",
      "Steps starting agent pos is [2, 3]\n",
      "Updated Q Table for agent at  [2, 3]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  2  with  q value 2.1870000000000003\n",
      "Updated Q Table for agent at  [2, 3]  for action  3  with  q value 0.0\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 2.43\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.9683000000000004\n",
      "Steps starting agent pos is [3, 2]\n",
      "Updated Q Table for agent at  [3, 2]  for action  0  with  q value 2.1870000000000003\n",
      "Updated Q Table for agent at  [3, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[2, 2]]\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.9683000000000004\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.9683000000000004\n",
      "equal states is , [[2, 3]]\n",
      "Steps starting agent pos is [2, 3]\n",
      "Updated Q Table for agent at  [2, 3]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  2  with  q value 1.7714700000000003\n",
      "Updated Q Table for agent at  [2, 3]  for action  3  with  q value 0.0\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.9683000000000004\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.5943230000000004\n",
      "Steps starting agent pos is [3, 2]\n",
      "Updated Q Table for agent at  [3, 2]  for action  0  with  q value 1.7714700000000003\n",
      "Updated Q Table for agent at  [3, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[2, 2]]\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.5943230000000004\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.5943230000000004\n",
      "equal states is , [[2, 3]]\n",
      "Steps starting agent pos is [2, 3]\n",
      "Updated Q Table for agent at  [2, 3]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  2  with  q value 1.4348907000000004\n",
      "Updated Q Table for agent at  [2, 3]  for action  3  with  q value 0.0\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.5943230000000004\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.2914016300000004\n",
      "Steps starting agent pos is [3, 2]\n",
      "Updated Q Table for agent at  [3, 2]  for action  0  with  q value 1.4348907000000004\n",
      "Updated Q Table for agent at  [3, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[2, 2]]\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.2914016300000004\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.2914016300000004\n",
      "equal states is , [[2, 3]]\n",
      "Steps starting agent pos is [2, 3]\n",
      "Updated Q Table for agent at  [2, 3]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  2  with  q value 1.1622614670000004\n",
      "Updated Q Table for agent at  [2, 3]  for action  3  with  q value 0.0\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.2914016300000004\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.0460353203000003\n",
      "Steps starting agent pos is [3, 2]\n",
      "Updated Q Table for agent at  [3, 2]  for action  0  with  q value 1.1622614670000004\n",
      "Updated Q Table for agent at  [3, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[2, 2]]\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.0460353203000003\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 1.0460353203000003\n",
      "equal states is , [[2, 3]]\n",
      "Steps starting agent pos is [2, 3]\n",
      "Updated Q Table for agent at  [2, 3]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  2  with  q value 0.9414317882700003\n",
      "Updated Q Table for agent at  [2, 3]  for action  3  with  q value 0.0\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 1.0460353203000003\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 0.8472886094430003\n",
      "Steps starting agent pos is [3, 2]\n",
      "Updated Q Table for agent at  [3, 2]  for action  0  with  q value 0.9414317882700003\n",
      "Updated Q Table for agent at  [3, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[2, 2]]\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 0.8472886094430003\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 0.8472886094430003\n",
      "equal states is , [[2, 3]]\n",
      "Steps starting agent pos is [2, 3]\n",
      "Updated Q Table for agent at  [2, 3]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 3]  for action  2  with  q value 0.7625597484987003\n",
      "Updated Q Table for agent at  [2, 3]  for action  3  with  q value 0.0\n",
      "Steps starting agent pos is [2, 2]\n",
      "Updated Q Table for agent at  [2, 2]  for action  0  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  1  with  q value 0.8472886094430003\n",
      "Updated Q Table for agent at  [2, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [2, 2]  for action  3  with  q value 0.6863037736488303\n",
      "Steps starting agent pos is [3, 2]\n",
      "Updated Q Table for agent at  [3, 2]  for action  0  with  q value 0.7625597484987003\n",
      "Updated Q Table for agent at  [3, 2]  for action  1  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  2  with  q value 0.0\n",
      "Updated Q Table for agent at  [3, 2]  for action  3  with  q value 0.0\n",
      "equal states is , [[2, 2]]\n",
      "6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "environment_q_learning_obj.qvalue_table"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{(0, 0): array([0., 0., 0., 0.]),\n",
       " (0, 1): array([0., 0., 0., 0.]),\n",
       " (0, 2): array([ 0.,  0.,  0., -1.]),\n",
       " (0, 3): array([0., 0., 0., 0.]),\n",
       " (0, 4): array([0., 0., 0., 0.]),\n",
       " (1, 0): array([0., 0., 0., 0.]),\n",
       " (1, 1): array([0., 0., 0., 0.]),\n",
       " (1, 2): array([0., 0., 0., 0.]),\n",
       " (1, 3): array([0., 0., 0., 0.]),\n",
       " (1, 4): array([0., 0., 0., 0.]),\n",
       " (2, 0): array([ 0., -1.,  0.,  0.]),\n",
       " (2, 1): array([0., 0., 0., 0.]),\n",
       " (2, 2): array([0.        , 0.84728861, 0.        , 0.68630377]),\n",
       " (2, 3): array([0.        , 0.        , 0.76255975, 0.        ]),\n",
       " (2, 4): array([0., 0., 0., 0.]),\n",
       " (3, 0): array([0., 0., 0., 0.]),\n",
       " (3, 1): array([0., 0., 0., 0.]),\n",
       " (3, 2): array([0.76255975, 0.        , 0.        , 0.        ]),\n",
       " (3, 3): array([0., 0., 0., 0.]),\n",
       " (3, 4): array([0., 0., 0., 0.]),\n",
       " (4, 0): array([0., 0., 0., 0.]),\n",
       " (4, 1): array([0., 0., 0., 0.]),\n",
       " (4, 2): array([0., 0., 0., 0.]),\n",
       " (4, 3): array([0., 0., 0., 0.]),\n",
       " (4, 4): array([0., 0., 0., 0.])}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9ec5a702473176f6b5ce6c18e1a25a3b92dc3c0568fd1d66af71c175925cbdff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}