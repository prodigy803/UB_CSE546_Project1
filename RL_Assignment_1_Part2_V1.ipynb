{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class environment_q_learning:\n",
    "    def __init__(self,type_of_env:str,epsilon:float,epsilon_decay:float,learning_rate:float,no_of_episodes:int,max_time_steps:int):\n",
    "\n",
    "        if len(max_time_steps) >= 10:\n",
    "            raise ValueError(\"Timesteps should be greater than of equal to 10\")\n",
    "        \n",
    "        if (epsilon_decay > 1) or (epsilon_decay < 0):\n",
    "            raise ValueError(\"Epsilon decay should be less than 1 and greater than 0\")\n",
    "        \n",
    "        if no_of_episodes < 1:\n",
    "            raise ValueError(\"No of Episodes should be atleast equal to 1\")\n",
    "\n",
    "        # No of number of states = 25 - Requirement 1\n",
    "        self.environment = np.zeros((5,5))\n",
    "        \n",
    "        self.max_timesteps = max_time_steps\n",
    "        self.current_time_steps = 0\n",
    "        \n",
    "        # This determines the exploitation vs exploration phase.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # this determines the reduction in epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # this determines how quickly the q values for a state are updated\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # this tells us the no_of_epsiodes during which we will determine the optimal q value\n",
    "        self.no_of_episodes = no_of_episodes\n",
    "        \n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "\n",
    "        self.environment[tuple(self.agent_current_pos)] = 1\n",
    "        self.environment[tuple(self.goal_pos)] = 0.5\n",
    "\n",
    "        # Collection of Rewards (the keys) and associated values (the states). -> Total No of Rewards = 4 -> Requirement 3\n",
    "        self.reward_states = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]}]\n",
    "        \n",
    "        # Setting the colors for the reward states in the environment.\n",
    "        for reward_state in self.reward_states:\n",
    "            for reward, position in reward_state.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        # Either Deterministic or stochastic.\n",
    "        self.environment_type = type_of_env\n",
    "\n",
    "        # This tracks the reward for the agent.\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Here we are essentially resetting all the values.\n",
    "        self.current_time_steps = 0\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "        \n",
    "        self.environment = np.zeros((5,5))\n",
    "\n",
    "        self.reward_states = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]}]\n",
    "        \n",
    "        for reward in self.reward_states:\n",
    "            for reward, position in reward.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        self.environment[self.agent_current_pos] = 1\n",
    "        self.environment[self.goal_pos] = 0.5\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        # We are checking wether the environment is deterministic or stochastic\n",
    "        if self.environment_type == 'deterministic':\n",
    "            # In Deterministic environments, there is no use for epsilon as all the actions are deterministic / greedy / pre-determined.\n",
    "\n",
    "            self.epsilon = 0\n",
    "            self.current_time_steps +=1\n",
    "\n",
    "            # The agent can take a maximum of 4 actions i.e Up, Down, Left or Right. -> Requirement 2\n",
    "            if action == 0:\n",
    "                print('Up')\n",
    "                self.agent_current_pos[0] -=1\n",
    "\n",
    "            elif action == 1:\n",
    "                print('Down')\n",
    "                self.agent_current_pos[0] +=1\n",
    "\n",
    "            elif action == 2:\n",
    "                print('Our Left or the Agents Right')\n",
    "                self.agent_current_pos[1] -=1\n",
    "\n",
    "            elif action == 3:\n",
    "                print('Our Right or the Agents Left')\n",
    "                self.agent_current_pos[1] +=1\n",
    "            \n",
    "            else:\n",
    "                print('Action was undefined')\n",
    "\n",
    "            # Here we are clipping the agents position to be in the environment (i.e if the agent goes out of env, we shall clip him to be inside the environment).\n",
    "            self.agent_current_pos = list(np.clip(self.agent_current_pos, 0, 4))\n",
    " \n",
    "            # Here we are calculating the reward (i.e. the cumulative reward) and deleting that reward state from the collection of the reward states.\n",
    "            breaker = False\n",
    "            for reward_state_counter in range(len(self.reward_states)):\n",
    "                for reward, state in self.reward_states[reward_state_counter].items():\n",
    "                    # if the reward state matches the agents, sum the cum reward and delete that particular reward state space.\n",
    "\n",
    "                    if state == self.agent_current_pos:\n",
    "                        self.cumulative_reward += reward\n",
    "                        del self.reward_states[reward_state_counter]\n",
    "                        breaker = True\n",
    "                        break\n",
    "\n",
    "                if breaker:\n",
    "                    break\n",
    "            \n",
    "            # We are now re-visualizing the environment\n",
    "            self.environment = np.zeros((5,5)) \n",
    "\n",
    "            for reward_state_counter in range(len(self.reward_states)):\n",
    "                for reward, position in self.reward_states[reward_state_counter].items():\n",
    "                    self.environment[tuple(position)] = reward\n",
    "                \n",
    "            self.environment[tuple(self.goal_pos)] = 0.5\n",
    "            self.environment[tuple(self.agent_current_pos)] = 1\n",
    "            \n",
    "            # if the agent has reached the final state then done\n",
    "            if (self.agent_current_pos == self.goal_pos) or (self.current_time_steps == self.max_timesteps):\n",
    "                done_or_not = True\n",
    "            \n",
    "            else:\n",
    "                done_or_not = False\n",
    "\n",
    "            return self.environment.flatten, self.cumulative_reward, done_or_not, self.current_time_steps\n",
    "\n",
    "        \n",
    "\n",
    "    def render(self):\n",
    "        plt.imshow(self.environment)\n",
    "        plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9ec5a702473176f6b5ce6c18e1a25a3b92dc3c0568fd1d66af71c175925cbdff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}