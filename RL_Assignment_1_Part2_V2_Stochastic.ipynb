{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# Major Change 1:\n",
    "    # Instead of fixed variable actions based on the optimal action chosen, the agent is free to choose from any available action  \n",
    "    # rather than those fixed variable actions.\n",
    "    # For example:\n",
    "    # My V1 had a case that If a agent chooses to go up, it has a equal likelihood of going down and left,\n",
    "    # Now instead of having down and left fixed, we are going to keep it such that agent can choose any from down, \n",
    "    # left and right (whatever is available for the agent to take)\n",
    "\n",
    "class environment_q_learning:\n",
    "    def __init__(self,type_of_env:str,gamma_disc: float, epsilon:float,epsilon_decay:float,learning_rate:float,no_of_episodes:int,max_time_steps:int):\n",
    "        \n",
    "        self.chain_of_states = []\n",
    "        self.chain_of_actions = []\n",
    "        self.chain_of_rewards = []\n",
    "        if max_time_steps < 10:\n",
    "            raise ValueError(\"Timesteps should be greater than of equal to 10\")\n",
    "        \n",
    "        if (epsilon_decay > 1) or (epsilon_decay < 0):\n",
    "            raise ValueError(\"Epsilon decay should be less than 1 and greater than 0\")\n",
    "        \n",
    "        if no_of_episodes < 1:\n",
    "            raise ValueError(\"No of Episodes should be atleast equal to 1\")\n",
    "\n",
    "        # No of number of states = 25 - Requirement 1\n",
    "        self.environment = np.zeros((5,5))\n",
    "\n",
    "        # No of actions an agent can take:\n",
    "        self.action_set_size = 4\n",
    "\n",
    "        self.gamma = gamma_disc\n",
    "        # Q Value Learning Table\n",
    "        # self. = np.zeros((len(self.environment.flatten())),self.action_set_size)\n",
    "        self.qvalue_table = {}\n",
    "        for i1 in range(5):\n",
    "            for i2 in range(5):\n",
    "                for i in np.zeros((25,4)):\n",
    "                    self.qvalue_table[(i1,i2)] = i\n",
    "        \n",
    "        self.max_timesteps = max_time_steps\n",
    "        self.current_time_steps = 0\n",
    "        \n",
    "        # This determines the exploitation vs exploration phase.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # this determines the reduction in epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # this determines how quickly the q values for a state are updated\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # this tells us the no_of_epsiodes during which we will determine the optimal q value\n",
    "        self.no_of_episodes = no_of_episodes\n",
    "        self.current_episode = 1\n",
    "        \n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "        self.done_or_not = False\n",
    "\n",
    "        self.environment[tuple(self.agent_current_pos)] = 1\n",
    "        self.environment[tuple(self.goal_pos)] = 0.5\n",
    "\n",
    "        # Collection of Rewards (the keys) and associated values (the states). -> Total No of Rewards = 4 -> Requirement 3\n",
    "        self.rewards = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]}]\n",
    "        self.reward_states = [(0,3),(3,0),(2,3),(3,2)]\n",
    "        # Setting the colors for the reward states in the environment.\n",
    "        for reward_state in self.rewards:\n",
    "            for reward, position in reward_state.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        # Either Deterministic or stochastic.\n",
    "        self.environment_type = type_of_env\n",
    "\n",
    "        # This tracks the reward for the agent.\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Here we are essentially resetting all the values.\n",
    "        self.current_time_steps = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.chain_of_states = []\n",
    "        self.chain_of_actions = []\n",
    "        self.chain_of_rewards = []\n",
    "\n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "        \n",
    "        self.environment = np.zeros((5,5))\n",
    "        \n",
    "        self.rewards = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]},{5:[4,4]}]\n",
    "        self.reward_states = [(0,3),(3,0),(2,3),(3,2),[4,4]]\n",
    "        \n",
    "        for reward in self.rewards:\n",
    "            for reward, position in reward.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        self.environment[self.agent_current_pos] = 1\n",
    "        self.environment[self.goal_pos] = 0.5\n",
    "    \n",
    "    def step(self):\n",
    "        # print(\"Steps starting agent pos is\", self.agent_current_pos)\n",
    "        # We are checking wether the environment is deterministic or stochastic\n",
    "        if self.environment_type == 'stochastic':\n",
    "            # In Deterministic environments, there is no use for epsilon as all the actions are deterministic / greedy / pre-determined.\n",
    "            \n",
    "            # self.epsilon = 0\n",
    "            self.current_time_steps +=1\n",
    "\n",
    "            all_possible_actions = self.get_all_possible_actions(self.agent_current_pos)\n",
    "            states_the_actions_lead_to = self.get_states_for_actions(all_possible_actions,self.agent_current_pos)\n",
    "            # all_possible_actions -> [1,3] like that a single list\n",
    "            # states_the_actions_lead_to -> [[1,0],[0,1]] -> states which the all_possible_actions lead to\n",
    "            # print(states_the_actions_lead_to)\n",
    "            \n",
    "            \n",
    "            # take a random_action:\n",
    "            if self.current_episode == 1:\n",
    "                \n",
    "                random_action = random.choice(all_possible_actions)\n",
    "                dict_temp = {}\n",
    "                for action, state in zip(all_possible_actions,states_the_actions_lead_to):\n",
    "                    dict_temp[action] = state\n",
    "                \n",
    "                self.agent_current_pos = dict_temp[random_action]\n",
    "                self.chain_of_states.append(dict_temp[random_action])\n",
    "                self.chain_of_actions.append(random_action)\n",
    "                self.chain_of_rewards.append(self.check_and_get_reward(dict_temp[random_action]))\n",
    "            \n",
    "            elif self.current_episode > 1:\n",
    "                # self.epsilon = 0\n",
    "                self.current_time_steps +=1\n",
    "\n",
    "                optimal_action, optimal_state,q_value_evaluated, q_values_evaluated = self.get_best_state_on_q_value(self.agent_current_pos,all_possible_actions,states_the_actions_lead_to)\n",
    "                choices = [i for i in states_the_actions_lead_to if i!=optimal_state]\n",
    "\n",
    "\n",
    "                print('Agent Current State, ',self.agent_current_pos)\n",
    "                print(\"All Possible Actions, \",all_possible_actions, \"and their corresponding resultant states are ,\",states_the_actions_lead_to)\n",
    "                \n",
    "                old_pos = self.agent_current_pos\n",
    "\n",
    "                if optimal_action == 0:\n",
    "                    print(\"Optimal Action is Up\")\n",
    "                    self.agent_current_pos = self.get_final_action(optimal_state,choices)\n",
    "\n",
    "                elif optimal_action == 1:\n",
    "                    print(\"Optimal Action is Down\")\n",
    "                    self.agent_current_pos = self.get_final_action(optimal_state,choices)\n",
    "\n",
    "                elif optimal_action == 2:\n",
    "                    print(\"Optimal Action is Our Left or the Agents Right\")\n",
    "                    self.agent_current_pos = self.get_final_action(optimal_state,choices)\n",
    "\n",
    "                elif optimal_action == 3:\n",
    "                    print(\"Optimal Action is Our Right or the Agents Left\")\n",
    "                    self.agent_current_pos = self.get_final_action(optimal_state,choices)\n",
    "\n",
    "                print('The Stocha Optimal State Selected is ',self.agent_current_pos, \"via \",old_pos,\" with q value \", q_value_evaluated, \" from \",q_values_evaluated)\n",
    "                \n",
    "                action_taken_finally = self.get_action_comparison(old_pos,self.agent_current_pos)\n",
    "\n",
    "                self.chain_of_states.append(self.agent_current_pos)\n",
    "                self.chain_of_actions.append(optimal_action)\n",
    "                self.chain_of_rewards.append(self.check_and_get_reward(self.agent_current_pos))\n",
    "\n",
    "            breaker = False\n",
    "            for reward_state_counter in range(len(self.rewards)):\n",
    "                for reward, state in self.rewards[reward_state_counter].items():\n",
    "                    # if the reward state matches the agents, sum the cum reward and delete that particular reward state space.\n",
    "\n",
    "                    if state == self.agent_current_pos:\n",
    "                        self.cumulative_reward += reward\n",
    "                        del self.rewards[reward_state_counter]\n",
    "                        breaker = True\n",
    "                        break\n",
    "\n",
    "                if breaker:\n",
    "                    break\n",
    "            \n",
    "            # We are now re-visualizing the environment\n",
    "            self.environment = np.zeros((5,5)) \n",
    "\n",
    "            for reward_state_counter in range(len(self.rewards)):\n",
    "                for reward, position in self.rewards[reward_state_counter].items():\n",
    "                    self.environment[tuple(position)] = reward\n",
    "                \n",
    "            self.environment[tuple(self.goal_pos)] = 0.5\n",
    "            self.environment[tuple(self.agent_current_pos)] = 1\n",
    "            \n",
    "            # if the agent has reached the final state then done\n",
    "            if (self.agent_current_pos == self.goal_pos) or (self.current_time_steps == self.max_timesteps):\n",
    "                self.done_or_not = True\n",
    "            \n",
    "            else:\n",
    "                self.done_or_not = False\n",
    "            # print('Done or not floag', done_or_not)\n",
    "            return self.environment.flatten, self.cumulative_reward, self.done_or_not, self.current_time_steps\n",
    "\n",
    "    # def get_final_action(self, action1, action2, action3):\n",
    "    def get_final_action(self, optimal_action,choices):\n",
    "\n",
    "        \"\"\"\n",
    "        This function gets the final action for the \"stochastic\" modeled environment.\n",
    "        \n",
    "        For example take Epsilon as 0.7\n",
    "        \n",
    "        Then when we generate the random no, and if it is in the range of 0 - 0.7 (or 0.69999), then we take random exploratory actions.\n",
    "        When we generate the random no, and if its is in the range of 0.7 - 1, then we take the greedy (or pre-determined) step in our case.\n",
    "        Please note that at the moment the steps are chosen arbritrarily and not based the next states (like if there is a reward waiting in the \n",
    "        next state, then in the current version of the environment, we are not bothering about that, we are still going ahead with predetermined states\n",
    "        with a chance of taking a random action).\n",
    "        \"\"\"\n",
    "        state_to_be_returned = []\n",
    "        random_n_number = random.uniform(0, 1)\n",
    "        # print('randn is', random_n_number)\n",
    "\n",
    "        random_action_proba = self.epsilon\n",
    "        old_pos = self.agent_current_pos\n",
    "\n",
    "        if random_n_number > random_action_proba:\n",
    "            return optimal_action\n",
    "            \n",
    "        else:\n",
    "            return random.choice(choices)\n",
    "\n",
    "    def get_action_comparison(self, old_pos, new_pos):\n",
    "        # This function tells us whether the final step the agent took after completing stochastic decision.\n",
    "\n",
    "        shift = [old_pos[i]-new_pos[i] for i in range(len(new_pos))]\n",
    "        # print(shift)\n",
    "        if shift == [-1,0]: \n",
    "            print('The Agent Ended Up Going Down to ,',new_pos)\n",
    "            return 0\n",
    "        elif shift == [1,0]:\n",
    "            print('The Agent Ended Up Going Up to ,',new_pos)\n",
    "            return 1\n",
    "        elif shift == [0,-1]:\n",
    "            print('The Agent Ended Up Going Right to ,',new_pos)\n",
    "            return 2\n",
    "        elif shift == [0,1]:\n",
    "            print('The Agent Ended Up Going Left to ,',new_pos)\n",
    "            return 3\n",
    "    \n",
    "    def get_best_state_on_q_value(self,current_state, all_possible_actions,states_the_actions_lead_to):\n",
    "        current_max = None\n",
    "        state_to_be_returned = None\n",
    "        action_to_be_returned = None\n",
    "        q_values_evaluated = None\n",
    "\n",
    "        for action,state in zip(all_possible_actions,states_the_actions_lead_to):\n",
    "            # print([4,4] in states_the_actions_lead_to)\n",
    "            if state == [4,4]:\n",
    "                return action, state,0, 0 \n",
    "\n",
    "            elif current_max == None:\n",
    "                current_max= self.qvalue_table[tuple(current_state)][action]\n",
    "                state_to_be_returned = state\n",
    "                action_to_be_returned = action\n",
    "                q_values_evaluated = self.qvalue_table[tuple(current_state)]\n",
    "\n",
    "            elif current_max != None:\n",
    "                max1 = self.qvalue_table[tuple(current_state)][action]\n",
    "                if max1 > current_max:\n",
    "                    current_max = max1\n",
    "                    state_to_be_returned = state\n",
    "                    action_to_be_returned = action\n",
    "                    q_values_evaluated = self.qvalue_table[tuple(current_state)]\n",
    "\n",
    "        return action_to_be_returned, state_to_be_returned, current_max, q_values_evaluated\n",
    "                \n",
    "\n",
    "\n",
    "    def get_state_based_on_action(self, action, state):\n",
    "        state_copy = list(state).copy()\n",
    "        if action == 0:\n",
    "            state_copy[0] -=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 1:\n",
    "            # print('Down')\n",
    "            state_copy[0] +=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 2:\n",
    "            # print('Our Left or the Agents Right')\n",
    "            state_copy[1] -=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 3:\n",
    "            # print('Our Right or the Agents Left')\n",
    "            state_copy[1] +=1\n",
    "            return state_copy\n",
    "\n",
    "    def check_and_get_reward(self, state_result):\n",
    "        # print('checking rewards for, ', state_result)\n",
    "        for i in range(len(self.rewards)):\n",
    "            for key, value in self.rewards[i].items():\n",
    "                if value == state_result:\n",
    "                    # self.rewards.pop(i)\n",
    "                    # print('found reward for state_result', state_result, value)\n",
    "                    return key\n",
    "\n",
    "        return 0\n",
    "\n",
    "    # def get_all_q_values_for_states(self,all_actions,agent_state):\n",
    "    def get_states_for_actions(self, all_actions, agent_state):\n",
    "    \n",
    "        temp_states = list(agent_state).copy() \n",
    "        # vals_to_be_returned = []\n",
    "        states_considered = []\n",
    "\n",
    "        for action_to_be_taken in all_actions:\n",
    "\n",
    "            action_taken = self.get_state_based_on_action(action_to_be_taken,agent_state)\n",
    "            \n",
    "            states_considered.append(action_taken)\n",
    "\n",
    "            agent_state = temp_states.copy()\n",
    "\n",
    "        return states_considered\n",
    "\n",
    "    def get_all_possible_actions(self,agent_current_pos):\n",
    "        \n",
    "        x_pos = agent_current_pos[0]\n",
    "        y_pos = agent_current_pos[1]\n",
    "\n",
    "        if (x_pos == 0) and (y_pos == 0):\n",
    "            return [1,3]\n",
    "        \n",
    "        elif (x_pos == 4) and (y_pos == 0):\n",
    "            return [0,3]\n",
    "        \n",
    "        elif (x_pos == 0) and (y_pos == 4):\n",
    "            return [1,2]\n",
    "        \n",
    "        elif (x_pos == 0) and (y_pos <= 3):\n",
    "            return [1,2,3]\n",
    "        \n",
    "        elif (x_pos <= 3) and (y_pos == 0):\n",
    "            return [0,1,3]\n",
    "        \n",
    "        elif (x_pos == 4) and (y_pos <= 3 ):\n",
    "            return [0,2,3]\n",
    "\n",
    "        elif (x_pos < 4) and (y_pos == 4 ):\n",
    "            return [0,2,1]\n",
    "\n",
    "        elif (x_pos >=1) and (x_pos<4) and (y_pos >= 1) and (y_pos <4):\n",
    "            return [0,1,2,3]\n",
    "\n",
    "        elif (x_pos==4) and (y_pos == 4):\n",
    "            return 'Yay youve won'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def render(self):\n",
    "        plt.imshow(self.environment)\n",
    "        plt.show()\n",
    "\n",
    "    def compute_q_values(self):\n",
    "        self.chain_of_actions.reverse(), self.chain_of_states.reverse(), self.chain_of_rewards.reverse()\n",
    "\n",
    "        for i in range(len(self.chain_of_rewards)-1):\n",
    "            # print(\"updating q value of \",self.chain_of_states[i+1])\n",
    "            \n",
    "            old_q_value = self.qvalue_table[tuple(self.chain_of_states[i+1])][self.chain_of_actions[i]]\n",
    "            \n",
    "            self.qvalue_table[tuple(self.chain_of_states[i+1])][self.chain_of_actions[i]] = old_q_value + self.learning_rate * (self.chain_of_rewards[i] + self.gamma * max(self.qvalue_table[tuple(self.chain_of_states[i])]) - old_q_value)\n",
    "\n",
    "            # if tuple(self.chain_of_states[i]) == (4,4):\n",
    "            #     print(tuple(self.chain_of_states[i]))\n",
    "\n",
    "    def train(self):\n",
    "        # done = False\n",
    "        \n",
    "        for i in range(self.no_of_episodes):\n",
    "            self.current_time_steps = 0\n",
    "            self.done_or_not = False\n",
    "            self.reset()\n",
    "            while not self.done_or_not:\n",
    "                # self.render()\n",
    "                # print(self.current_episode)\n",
    "                observation, reward, self.done_or_not, _ = self.step()\n",
    "                if self.done_or_not:\n",
    "                    print('Flag got set')\n",
    "                    print(reward)\n",
    "                # if self.done_or_not:\n",
    "                #     # print('flag got set')\n",
    "                #     # print(reward)\n",
    "                #     print(self.current_time_steps)\n",
    "                #     break\n",
    "            # self.render()\n",
    "            print(self.current_time_steps) \n",
    "            self.compute_q_values()\n",
    "            \n",
    "            # print('Episode Completed')\n",
    "            # print('----')\n",
    "            self.current_episode+=1\n",
    "            # pprint(self.qvalue_table)       \n",
    "\n",
    "environment_q_learning_obj = environment_q_learning(type_of_env = 'stochastic',gamma_disc=0.9,epsilon=0.2,epsilon_decay=0.001,learning_rate=0.5,no_of_episodes=10,max_time_steps=1000)\n",
    "environment_q_learning_obj.train()\n",
    "pprint(environment_q_learning_obj.qvalue_table)\n",
    "# pprint(environment_q_learning_obj.chain_of_states)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Flag got set\n",
      "10\n",
      "82\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 0]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Right to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [0, 1]  with q value  0.8341301293945313  from  [0.         0.83413013 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  1.2783603515625002  from  [0.15532078 0.67280625 0.         1.27836035]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  1.988560546875  from  [0.75703658 1.98856055 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  2.52515625  from  [1.0308623  2.52515625 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  1.3921875  from  [0.87336773 0.72962292 0.         1.3921875 ]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [4, 3] via  [3, 3]  with q value  1.6875  from  [0.04613203 0.         0.85429688 1.6875    ]\n",
      "The Agent Ended Up Going Down to , [4, 3]\n",
      "Agent Current State,  [4, 3]\n",
      "All Possible Actions,  [0, 2, 3] and their corresponding resultant states are , [[3, 3], [4, 2], [4, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [4, 3]  with q value  0  from  0\n",
      "The Agent Ended Up Going Right to , [4, 4]\n",
      "Flag got set\n",
      "8\n",
      "16\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [1, 0]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Right to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  1.7901909667968752  from  [0.15532078 0.67280625 0.         1.79019097]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  2.5578017578125003  from  [0.75703658 2.55780176 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  3.4744921875  from  [1.0308623  3.47449219 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  1.58203125  from  [0.87336773 0.72962292 0.         1.58203125]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  1.96875  from  [0.04613203 0.         0.85429688 1.96875   ]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "8\n",
      "24\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 0]  with q value  0.5619608226013184  from  [0.         0.56196082 0.         0.        ]\n",
      "The Agent Ended Up Going Right to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 2] via  [0, 1]  with q value  1.2226509997558597  from  [0.       1.222651 0.       0.      ]\n",
      "The Agent Ended Up Going Right to , [0, 2]\n",
      "Agent Current State,  [0, 2]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 2], [0, 1], [0, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [0, 2]  with q value  1.0308623005371094  from  [0.         1.0308623  0.23298117 0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  3.13931689453125  from  [0.75703658 3.13931689 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [2, 2]  with q value  4.1342578125  from  [1.0308623  4.13425781 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Up to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 2] via  [1, 2]  with q value  3.13931689453125  from  [0.75703658 3.13931689 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 2]\n",
      "Agent Current State,  [0, 2]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 2], [0, 1], [0, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 2]  with q value  1.0308623005371094  from  [0.         1.0308623  0.23298117 0.        ]\n",
      "The Agent Ended Up Going Left to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [0, 1]  with q value  1.2226509997558597  from  [0.       1.222651 0.       0.      ]\n",
      "The Agent Ended Up Going Down to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [2, 1] via  [1, 1]  with q value  2.3077880859375  from  [0.15532078 0.67280625 0.         2.30778809]\n",
      "The Agent Ended Up Going Down to , [2, 1]\n",
      "Agent Current State,  [2, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 1], [3, 1], [2, 0], [2, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [2, 1]  with q value  1.3157226562500002  from  [0.34515729 0.35880469 0.07265795 1.31572266]\n",
      "The Agent Ended Up Going Right to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  4.1342578125  from  [1.0308623  4.13425781 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  1.993359375  from  [0.87336773 0.72962292 0.         1.99335938]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  2.671875  from  [0.04613203 0.         0.85429688 2.671875  ]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "8\n",
      "28\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  0.5619608226013184  from  [0.         0.56196082 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  0.6176571245040894  from  [0.61765712 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  0.5619608226013184  from  [0.         0.56196082 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  0.6176571245040894  from  [0.61765712 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  0.5619608226013184  from  [0.         0.56196082 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [1, 0]  with q value  0.6176571245040894  from  [0.61765712 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Right to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  2.3986105224609373  from  [0.15532078 0.67280625 0.         2.39861052]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  2.5602260533702585  from  [0.75703658 2.56022605 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 1] via  [2, 2]  with q value  3.3072031602994536  from  [1.0308623  3.30720316 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Left to , [2, 1]\n",
      "Agent Current State,  [2, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 1], [3, 1], [2, 0], [2, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [2, 1]  with q value  2.76603662109375  from  [0.34515729 0.35880469 0.07265795 2.76603662]\n",
      "The Agent Ended Up Going Up to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  2.3986105224609373  from  [0.15532078 0.67280625 0.         2.39861052]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 3] via  [1, 2]  with q value  2.5602260533702585  from  [0.75703658 2.56022605 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Right to , [1, 3]\n",
      "Agent Current State,  [1, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 3], [2, 3], [1, 2], [1, 4]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [2, 3] via  [1, 3]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Down to , [2, 3]\n",
      "Agent Current State,  [2, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 3], [3, 3], [2, 2], [2, 4]]\n",
      "Optimal Action is Our Left or the Agents Right\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [2, 3]  with q value  0.4100625  from  [0.        0.        0.4100625 0.       ]\n",
      "The Agent Ended Up Going Left to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  3.3072031602994536  from  [1.0308623  3.30720316 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  2.4837890625  from  [0.87336773 0.72962292 0.         2.48378906]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  3.3046875  from  [0.04613203 0.         0.85429688 3.3046875 ]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "11\n",
      "36\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  0.9377644603169191  from  [0.         0.93776446 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  0.9658003647748343  from  [0.96580036 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 0]  with q value  0.9377644603169191  from  [0.         0.93776446 0.         0.        ]\n",
      "The Agent Ended Up Going Right to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [0, 1]  with q value  1.650951300902845  from  [0.        1.6509513 0.        0.       ]\n",
      "The Agent Ended Up Going Down to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  2.3541936423790695  from  [0.15532078 0.67280625 0.         2.35419364]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  2.6968082625444416  from  [0.75703658 2.69680826 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  3.3214144421045058  from  [1.0308623  3.32141444 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  2.9346679687500004  from  [0.87336773 0.72962292 0.         2.93466797]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  3.76171875  from  [0.04613203 0.         0.85429688 3.76171875]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "8\n",
      "20\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 0]  with q value  1.3919847245458117  from  [0.         1.39198472 0.         0.        ]\n",
      "The Agent Ended Up Going Right to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [0, 1]  with q value  2.051338876416338  from  [0.         2.05133888 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  2.724140502144256  from  [0.15532078 0.67280625 0.         2.7241405 ]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [1, 2]  with q value  3.4378748465660474  from  [0.75703658 3.43787485 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Left to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  2.724140502144256  from  [0.15532078 0.67280625 0.         2.7241405 ]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  3.4378748465660474  from  [0.75703658 3.43787485 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [2, 2]  with q value  4.643268256208503  from  [1.0308623  4.64326826 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Up to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  3.4378748465660474  from  [0.75703658 3.43787485 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  4.643268256208503  from  [1.0308623  4.64326826 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 1] via  [3, 2]  with q value  3.2945800781250005  from  [0.87336773 0.72962292 0.         3.29458008]\n",
      "The Agent Ended Up Going Left to , [3, 1]\n",
      "Agent Current State,  [3, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 1], [4, 1], [3, 0], [3, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [3, 1]  with q value  0.79734375  from  [0.         0.10379707 0.         0.79734375]\n",
      "The Agent Ended Up Going Right to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  3.2945800781250005  from  [0.87336773 0.72962292 0.         3.29458008]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  4.060546875  from  [0.04613203 0.         0.85429688 4.06054688]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Up to , [2, 4]\n",
      "Agent Current State,  [2, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[1, 4], [2, 3], [3, 4]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [1, 4] via  [2, 4]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Up to , [1, 4]\n",
      "Agent Current State,  [1, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[0, 4], [1, 3], [2, 4]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [2, 4] via  [1, 4]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Down to , [2, 4]\n",
      "Agent Current State,  [2, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[1, 4], [2, 3], [3, 4]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [1, 4] via  [2, 4]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Up to , [1, 4]\n",
      "Agent Current State,  [1, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[0, 4], [1, 3], [2, 4]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [1, 3] via  [1, 4]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Left to , [1, 3]\n",
      "Agent Current State,  [1, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 3], [2, 3], [1, 2], [1, 4]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 3] via  [1, 3]  with q value  2.4982900011326636  from  [2.49829 0.      0.      0.     ]\n",
      "The Agent Ended Up Going Up to , [0, 3]\n",
      "Agent Current State,  [0, 3]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 3], [0, 2], [0, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 3] via  [0, 3]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Down to , [1, 3]\n",
      "Agent Current State,  [1, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 3], [2, 3], [1, 2], [1, 4]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 3] via  [1, 3]  with q value  2.4982900011326636  from  [2.49829 0.      0.      0.     ]\n",
      "The Agent Ended Up Going Up to , [0, 3]\n",
      "Agent Current State,  [0, 3]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 3], [0, 2], [0, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 2] via  [0, 3]  with q value  0.0  from  [0. 0. 0. 0.]\n",
      "The Agent Ended Up Going Left to , [0, 2]\n",
      "Agent Current State,  [0, 2]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 2], [0, 1], [0, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [0, 2]  with q value  1.790224852022598  from  [0.         1.79022485 0.23298117 0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  3.4378748465660474  from  [0.75703658 3.43787485 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  4.643268256208503  from  [1.0308623  4.64326826 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [4, 2] via  [3, 2]  with q value  3.2945800781250005  from  [0.87336773 0.72962292 0.         3.29458008]\n",
      "The Agent Ended Up Going Down to , [4, 2]\n",
      "Agent Current State,  [4, 2]\n",
      "All Possible Actions,  [0, 2, 3] and their corresponding resultant states are , [[3, 2], [4, 1], [4, 3]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [4, 2]  with q value  0.9966796875000001  from  [0.99667969 0.         0.15569561 0.51257812]\n",
      "The Agent Ended Up Going Up to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  3.2945800781250005  from  [0.87336773 0.72962292 0.         3.29458008]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  4.060546875  from  [0.04613203 0.         0.85429688 4.06054688]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "7\n",
      "60\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.3919847245458117  from  [0.         1.39198472 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.1092933084330325  from  [1.10929331 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.3919847245458117  from  [0.         1.39198472 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.1092933084330325  from  [1.10929331 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.3919847245458117  from  [0.         1.39198472 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.1092933084330325  from  [1.10929331 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.3919847245458117  from  [0.         1.39198472 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [2, 0] via  [1, 0]  with q value  1.1092933084330325  from  [1.10929331 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 0]\n",
      "Agent Current State,  [2, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[1, 0], [3, 0], [2, 1]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [2, 1] via  [2, 0]  with q value  0.161462109375  from  [0.         0.         0.         0.16146211]\n",
      "The Agent Ended Up Going Right to , [2, 1]\n",
      "Agent Current State,  [2, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 1], [3, 1], [2, 0], [2, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [2, 1]  with q value  2.4095852423575383  from  [0.34515729 0.35880469 0.07265795 2.40958524]\n",
      "The Agent Ended Up Going Right to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  3.7322517944421287  from  [1.0308623  3.73225179 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  2.1720411590781543  from  [0.87336773 0.72962292 0.         2.17204116]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [3, 3]  with q value  3.27725690613618  from  [0.04613203 0.         0.85429688 3.27725691]\n",
      "The Agent Ended Up Going Left to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  2.1720411590781543  from  [0.87336773 0.72962292 0.         2.17204116]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  3.27725690613618  from  [0.04613203 0.         0.85429688 3.27725691]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "8\n",
      "32\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.0851406226803013  from  [0.         1.08514062 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.0474841270009434  from  [1.04748413 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.0851406226803013  from  [0.         1.08514062 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.0474841270009434  from  [1.04748413 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.0851406226803013  from  [0.         1.08514062 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.0474841270009434  from  [1.04748413 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.0851406226803013  from  [0.         1.08514062 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.0474841270009434  from  [1.04748413 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.0851406226803013  from  [0.         1.08514062 0.         0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.0474841270009434  from  [1.04748413 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 0]  with q value  1.0851406226803013  from  [0.         1.08514062 0.         0.        ]\n",
      "The Agent Ended Up Going Right to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [0, 1]  with q value  2.299950704120674  from  [0.        2.2999507 0.        0.       ]\n",
      "The Agent Ended Up Going Left to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 0]  with q value  1.0851406226803013  from  [0.         1.08514062 0.         0.        ]\n",
      "The Agent Ended Up Going Right to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 1] via  [0, 1]  with q value  2.299950704120674  from  [0.        2.2999507 0.        0.       ]\n",
      "The Agent Ended Up Going Down to , [1, 1]\n",
      "Agent Current State,  [1, 1]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 1], [2, 1], [1, 0], [1, 2]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [1, 1]  with q value  2.8317361464722337  from  [0.15532078 0.67280625 0.         2.83173615]\n",
      "The Agent Ended Up Going Right to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  3.0469430910209514  from  [0.75703658 3.04694309 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  4.52299308189301  from  [1.0308623  4.52299308 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  2.5708159659376575  from  [0.87336773 0.72962292 0.         2.57081597]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  2.835746031298563  from  [0.04613203 0.         0.85429688 2.83574603]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "8\n",
      "40\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 0] via  [0, 0]  with q value  1.1498798991941181  from  [0.        1.1498799 0.        0.       ]\n",
      "The Agent Ended Up Going Down to , [1, 0]\n",
      "Agent Current State,  [1, 0]\n",
      "All Possible Actions,  [0, 1, 3] and their corresponding resultant states are , [[0, 0], [2, 0], [1, 1]]\n",
      "Optimal Action is Up\n",
      "The Stocha Optimal State Selected is  [0, 0] via  [1, 0]  with q value  1.1100003615247815  from  [1.11000036 0.         0.         0.        ]\n",
      "The Agent Ended Up Going Up to , [0, 0]\n",
      "Agent Current State,  [0, 0]\n",
      "All Possible Actions,  [1, 3] and their corresponding resultant states are , [[1, 0], [0, 1]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 1] via  [0, 0]  with q value  1.1498798991941181  from  [0.        1.1498799 0.        0.       ]\n",
      "The Agent Ended Up Going Right to , [0, 1]\n",
      "Agent Current State,  [0, 1]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 1], [0, 0], [0, 2]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [0, 2] via  [0, 1]  with q value  2.0381483333475776  from  [0.         2.03814833 0.         0.        ]\n",
      "The Agent Ended Up Going Right to , [0, 2]\n",
      "Agent Current State,  [0, 2]\n",
      "All Possible Actions,  [1, 2, 3] and their corresponding resultant states are , [[1, 2], [0, 1], [0, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [1, 2] via  [0, 2]  with q value  2.3869406387868874  from  [0.         2.38694064 0.23298117 0.        ]\n",
      "The Agent Ended Up Going Down to , [1, 2]\n",
      "Agent Current State,  [1, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[0, 2], [2, 2], [1, 1], [1, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [2, 2] via  [1, 2]  with q value  3.78572269246768  from  [0.75703658 3.78572269 0.07265795 0.        ]\n",
      "The Agent Ended Up Going Down to , [2, 2]\n",
      "Agent Current State,  [2, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[1, 2], [3, 2], [2, 1], [2, 3]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [2, 2]  with q value  5.027224771016009  from  [1.0308623  5.02722477 0.67280625 1.68452812]\n",
      "The Agent Ended Up Going Down to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  2.812729400154451  from  [0.87336773 0.72962292 0.         2.8127294 ]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 2] via  [3, 3]  with q value  3.3940475937458263  from  [0.04613203 0.         0.85429688 3.39404759]\n",
      "The Agent Ended Up Going Left to , [3, 2]\n",
      "Agent Current State,  [3, 2]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 2], [4, 2], [3, 1], [3, 3]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 3] via  [3, 2]  with q value  2.812729400154451  from  [0.87336773 0.72962292 0.         2.8127294 ]\n",
      "The Agent Ended Up Going Right to , [3, 3]\n",
      "Agent Current State,  [3, 3]\n",
      "All Possible Actions,  [0, 1, 2, 3] and their corresponding resultant states are , [[2, 3], [4, 3], [3, 2], [3, 4]]\n",
      "Optimal Action is Our Right or the Agents Left\n",
      "The Stocha Optimal State Selected is  [3, 4] via  [3, 3]  with q value  3.3940475937458263  from  [0.04613203 0.         0.85429688 3.39404759]\n",
      "The Agent Ended Up Going Right to , [3, 4]\n",
      "Agent Current State,  [3, 4]\n",
      "All Possible Actions,  [0, 2, 1] and their corresponding resultant states are , [[2, 4], [3, 3], [4, 4]]\n",
      "Optimal Action is Down\n",
      "The Stocha Optimal State Selected is  [4, 4] via  [3, 4]  with q value  0  from  0\n",
      "The Agent Ended Up Going Down to , [4, 4]\n",
      "Flag got set\n",
      "8\n",
      "24\n",
      "{(0, 0): array([0.        , 1.66854711, 0.        , 0.        ]),\n",
      " (0, 1): array([0.        , 2.43023814, 0.        , 0.        ]),\n",
      " (0, 2): array([0.        , 3.13591995, 0.23298117, 0.        ]),\n",
      " (0, 3): array([0.        , 1.31668686, 0.        , 0.        ]),\n",
      " (0, 4): array([0., 0., 0., 0.]),\n",
      " (1, 0): array([1.30584638, 0.        , 0.        , 0.        ]),\n",
      " (1, 1): array([0.15532078, 0.67280625, 0.        , 3.11944328]),\n",
      " (1, 2): array([0.75703658, 4.31655473, 0.07265795, 0.        ]),\n",
      " (1, 3): array([0.95875933, 0.        , 0.        , 0.        ]),\n",
      " (1, 4): array([0.30308779, 0.        , 0.        , 0.        ]),\n",
      " (2, 0): array([0.        , 0.        , 0.        , 1.53879383]),\n",
      " (2, 1): array([0.34515729, 0.35880469, 0.07265795, 3.24013951]),\n",
      " (2, 2): array([1.0308623 , 5.38598529, 0.67280625, 1.68452812]),\n",
      " (2, 3): array([0.        , 0.        , 2.21842222, 0.        ]),\n",
      " (2, 4): array([0.23346389, 0.        , 0.        , 0.        ]),\n",
      " (3, 0): array([0.        , 0.        , 0.        , 0.04670868]),\n",
      " (3, 1): array([0.        , 0.10379707, 0.        , 1.67509532]),\n",
      " (3, 2): array([0.87336773, 0.72962292, 0.        , 3.04971756]),\n",
      " (3, 3): array([0.04613203, 0.        , 0.85429688, 3.30946715]),\n",
      " (3, 4): array([0.        , 4.69574953, 0.50625   , 0.        ]),\n",
      " (4, 0): array([-0.47898109,  0.        ,  0.        ,  0.15569561]),\n",
      " (4, 1): array([0.20503125, 0.        , 0.09341736, 0.23066016]),\n",
      " (4, 2): array([2.09925659, 0.        , 0.15569561, 0.51257812]),\n",
      " (4, 3): array([0.759375  , 0.        , 0.15377344, 2.5       ]),\n",
      " (4, 4): array([0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9ec5a702473176f6b5ce6c18e1a25a3b92dc3c0568fd1d66af71c175925cbdff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}