{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "class environment_q_learning:\n",
    "    def __init__(self,type_of_env:str,gamma_disc: float, epsilon:float,epsilon_decay:float,learning_rate:float,no_of_episodes:int,max_time_steps:int):\n",
    "        \n",
    "        self.chain_of_states = []\n",
    "        self.chain_of_actions = []\n",
    "        self.chain_of_rewards = []\n",
    "        if max_time_steps < 10:\n",
    "            raise ValueError(\"Timesteps should be greater than of equal to 10\")\n",
    "        \n",
    "        if (epsilon_decay > 1) or (epsilon_decay < 0):\n",
    "            raise ValueError(\"Epsilon decay should be less than 1 and greater than 0\")\n",
    "        \n",
    "        if no_of_episodes < 1:\n",
    "            raise ValueError(\"No of Episodes should be atleast equal to 1\")\n",
    "\n",
    "        # No of number of states = 25 - Requirement 1\n",
    "        self.environment = np.zeros((5,5))\n",
    "\n",
    "        # No of actions an agent can take:\n",
    "        self.action_set_size = 4\n",
    "\n",
    "        self.gamma = gamma_disc\n",
    "        # Q Value Learning Table\n",
    "        # self. = np.zeros((len(self.environment.flatten())),self.action_set_size)\n",
    "        self.qvalue_table = {}\n",
    "        for i1 in range(5):\n",
    "            for i2 in range(5):\n",
    "                for i in np.zeros((25,4)):\n",
    "                    self.qvalue_table[(i1,i2)] = i\n",
    "        \n",
    "        self.max_timesteps = max_time_steps\n",
    "        self.current_time_steps = 0\n",
    "        \n",
    "        # This determines the exploitation vs exploration phase.\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # this determines the reduction in epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # this determines how quickly the q values for a state are updated\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # this tells us the no_of_epsiodes during which we will determine the optimal q value\n",
    "        self.no_of_episodes = no_of_episodes\n",
    "        self.current_episode = 1\n",
    "        \n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "        self.done_or_not = False\n",
    "\n",
    "        self.environment[tuple(self.agent_current_pos)] = 1\n",
    "        self.environment[tuple(self.goal_pos)] = 0.5\n",
    "\n",
    "        # Collection of Rewards (the keys) and associated values (the states). -> Total No of Rewards = 4 -> Requirement 3\n",
    "        self.rewards = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]}]\n",
    "        self.reward_states = [(0,3),(3,0),(2,3),(3,2)]\n",
    "        # Setting the colors for the reward states in the environment.\n",
    "        for reward_state in self.rewards:\n",
    "            for reward, position in reward_state.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        # Either Deterministic or stochastic.\n",
    "        self.environment_type = type_of_env\n",
    "\n",
    "        # This tracks the reward for the agent.\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Here we are essentially resetting all the values.\n",
    "        self.current_time_steps = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.chain_of_states = []\n",
    "        self.chain_of_actions = []\n",
    "        self.chain_of_rewards = []\n",
    "\n",
    "        self.goal_pos = [4,4]\n",
    "        self.agent_current_pos = [0,0]\n",
    "        \n",
    "        self.environment = np.zeros((5,5))\n",
    "        \n",
    "        self.rewards = [{-1:[0,3]},{-1:[3,0]},{3:[2,3]},{3:[3,2]},{10:[4,4]}]\n",
    "        self.reward_states = [(0,3),(3,0),(2,3),(3,2),[4,4]]\n",
    "        \n",
    "        for reward in self.rewards:\n",
    "            for reward, position in reward.items():\n",
    "                self.environment[tuple(position)] = reward\n",
    "\n",
    "        self.environment[self.agent_current_pos] = 1\n",
    "        self.environment[self.goal_pos] = 0.5\n",
    "    \n",
    "    def step(self):\n",
    "        # print(\"Steps starting agent pos is\", self.agent_current_pos)\n",
    "        # We are checking wether the environment is deterministic or stochastic\n",
    "        if self.environment_type == 'deterministic':\n",
    "            # In Deterministic environments, there is no use for epsilon as all the actions are deterministic / greedy / pre-determined.\n",
    "            \n",
    "            self.epsilon = 0\n",
    "            self.current_time_steps +=1\n",
    "\n",
    "            all_possible_actions = self.get_all_possible_actions(self.agent_current_pos)\n",
    "            states_the_actions_lead_to = self.get_states_for_actions(all_possible_actions,self.agent_current_pos)\n",
    "            # all_possible_actions -> [1,3] like that a single list\n",
    "            # states_the_actions_lead_to -> [[1,0],[0,1]] -> states which the all_possible_actions lead to\n",
    "            \n",
    "            \n",
    "            # take a random_action:\n",
    "            if self.current_episode == 1:\n",
    "                \n",
    "                random_action = random.choice(all_possible_actions)\n",
    "                dict_temp = {}\n",
    "                for action, state in zip(all_possible_actions,states_the_actions_lead_to):\n",
    "                    dict_temp[action] = state\n",
    "                \n",
    "                self.agent_current_pos = dict_temp[random_action]\n",
    "                self.chain_of_states.append(dict_temp[random_action])\n",
    "                self.chain_of_actions.append(random_action)\n",
    "                self.chain_of_rewards.append(self.check_and_get_reward(dict_temp[random_action]))\n",
    "            \n",
    "            elif self.current_episode == 1:\n",
    "                optimal_action, optimal_state = self.get_best_state_on_q_value(all_possible_actions,states_the_actions_lead_to)\n",
    "                self.agent_current_pos = optimal_state\n",
    "                self.chain_of_states.append(optimal_state)\n",
    "                self.chain_of_actions.append(optimal_action)\n",
    "                self.chain_of_rewards.append(self.check_and_get_reward(optimal_state))\n",
    "\n",
    "            breaker = False\n",
    "            for reward_state_counter in range(len(self.rewards)):\n",
    "                for reward, state in self.rewards[reward_state_counter].items():\n",
    "                    # if the reward state matches the agents, sum the cum reward and delete that particular reward state space.\n",
    "\n",
    "                    if state == self.agent_current_pos:\n",
    "                        self.cumulative_reward += reward\n",
    "                        del self.rewards[reward_state_counter]\n",
    "                        breaker = True\n",
    "                        break\n",
    "\n",
    "                if breaker:\n",
    "                    break\n",
    "            \n",
    "            # We are now re-visualizing the environment\n",
    "            self.environment = np.zeros((5,5)) \n",
    "\n",
    "            for reward_state_counter in range(len(self.rewards)):\n",
    "                for reward, position in self.rewards[reward_state_counter].items():\n",
    "                    self.environment[tuple(position)] = reward\n",
    "                \n",
    "            self.environment[tuple(self.goal_pos)] = 0.5\n",
    "            self.environment[tuple(self.agent_current_pos)] = 1\n",
    "            \n",
    "            # if the agent has reached the final state then done\n",
    "            if (self.agent_current_pos == self.goal_pos) or (self.current_time_steps == self.max_timesteps):\n",
    "                self.done_or_not = True\n",
    "            \n",
    "            else:\n",
    "                self.done_or_not = False\n",
    "            # print('Done or not floag', done_or_not)\n",
    "            return self.environment.flatten, self.cumulative_reward, self.done_or_not, self.current_time_steps\n",
    "    \n",
    "    def get_best_state_on_q_value(self,all_possible_actions,states_the_actions_lead_to):\n",
    "        current_max = None\n",
    "        state_to_be_returned = None\n",
    "        action_to_be_returned = None\n",
    "        for action,state in zip(all_possible_actions,states_the_actions_lead_to):\n",
    "            if current_max == None:\n",
    "                current_max= max(self.qvalue_table[tuple(state)][action])\n",
    "                state_to_be_returned = state\n",
    "                action_to_be_returned = action\n",
    "\n",
    "            elif current_max != None:\n",
    "                max1 = self.qvalue_table[tuple(state)][action]\n",
    "                if max1 > current_max:\n",
    "                    current_max = max1\n",
    "                    state_to_be_returned = state\n",
    "                    action_to_be_returned = action\n",
    "        return action_to_be_returned, state_to_be_returned\n",
    "\n",
    "    def get_best_state_on_reward(self,all_possible_actions, states_the_actions_lead_to):\n",
    "        \n",
    "        for action,state in zip(all_possible_actions,states_the_actions_lead_to):\n",
    "            \n",
    "            for i in range(len(self.rewards)):\n",
    "                for key, value in self.rewards[i].items():\n",
    "                    if value == state:\n",
    "                        return action, state\n",
    "        return None, None\n",
    "                \n",
    "\n",
    "\n",
    "    def get_state_based_on_action(self, action, state):\n",
    "        state_copy = list(state).copy()\n",
    "        if action == 0:\n",
    "            # print('Up')\n",
    "            state_copy[0] -=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 1:\n",
    "            # print('Down')\n",
    "            state_copy[0] +=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 2:\n",
    "            # print('Our Left or the Agents Right')\n",
    "            state_copy[1] -=1\n",
    "            return state_copy\n",
    "\n",
    "        elif action == 3:\n",
    "            # print('Our Right or the Agents Left')\n",
    "            state_copy[1] +=1\n",
    "            return state_copy\n",
    "\n",
    "    def check_and_get_reward(self, state_result):\n",
    "        # print('checking rewards for, ', state_result)\n",
    "        for i in range(len(self.rewards)):\n",
    "            for key, value in self.rewards[i].items():\n",
    "                if value == state_result:\n",
    "                    # self.rewards.pop(i)\n",
    "                    # print('found reward for state_result', state_result, value)\n",
    "                    return key\n",
    "\n",
    "        return 0\n",
    "\n",
    "    # def get_all_q_values_for_states(self,all_actions,agent_state):\n",
    "    def get_states_for_actions(self, all_actions, agent_state):\n",
    "    \n",
    "        temp_states = list(agent_state).copy() \n",
    "        # vals_to_be_returned = []\n",
    "        states_considered = []\n",
    "\n",
    "        for action_to_be_taken in all_actions:\n",
    "\n",
    "            action_taken = self.get_state_based_on_action(action_to_be_taken,agent_state)\n",
    "            \n",
    "            states_considered.append(action_taken)\n",
    "\n",
    "            agent_state = temp_states.copy()\n",
    "\n",
    "        return states_considered\n",
    "\n",
    "    def get_all_possible_actions(self,agent_current_pos):\n",
    "        \n",
    "        x_pos = agent_current_pos[0]\n",
    "        y_pos = agent_current_pos[1]\n",
    "\n",
    "        if (x_pos == 0) and (y_pos == 0):\n",
    "            return [1,3]\n",
    "        \n",
    "        elif (x_pos == 4) and (y_pos == 0):\n",
    "            return [0,3]\n",
    "        \n",
    "        elif (x_pos == 0) and (y_pos == 4):\n",
    "            return [1,2]\n",
    "        \n",
    "        elif (x_pos == 0) and (y_pos <= 3):\n",
    "            return [1,2,3]\n",
    "        \n",
    "        elif (x_pos <= 3) and (y_pos == 0):\n",
    "            return [0,1,3]\n",
    "        \n",
    "        elif (x_pos == 4) and (y_pos <= 3 ):\n",
    "            return [0,2,3]\n",
    "\n",
    "        elif (x_pos < 4) and (y_pos == 4 ):\n",
    "            return [0,2,1]\n",
    "\n",
    "        elif (x_pos >=1) and (x_pos<4) and (y_pos >= 1) and (y_pos <4):\n",
    "            return [0,1,2,3]\n",
    "\n",
    "        elif (x_pos==4) and (y_pos == 4):\n",
    "            return 'Yay youve won'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def render(self):\n",
    "        plt.imshow(self.environment)\n",
    "        plt.show()\n",
    "\n",
    "    def compute_q_values(self):\n",
    "        self.chain_of_actions.reverse(), self.chain_of_states.reverse(), self.chain_of_rewards.reverse()\n",
    "\n",
    "        for i in range(len(self.chain_of_rewards)-1):\n",
    "            # print(\"updating q value of \",self.chain_of_states[i+1])\n",
    "            self.qvalue_table[tuple(self.chain_of_states[i+1])][self.chain_of_actions[i]]\n",
    "            \n",
    "            old_q_value = self.qvalue_table[tuple(self.chain_of_states[i+1])][self.chain_of_actions[i]]\n",
    "            \n",
    "            self.qvalue_table[tuple(self.chain_of_states[i+1])][self.chain_of_actions[i]] = old_q_value + self.learning_rate *(self.chain_of_rewards[i] + self.gamma * max(self.qvalue_table[tuple(self.chain_of_states[i])]) - old_q_value)\n",
    "\n",
    "            if tuple(self.chain_of_states[i+1]) == (4,4):\n",
    "                print(tuple(self.chain_of_states[i+1]))\n",
    "            # print(\"new_q_value is \",self.qvalue_table[tuple(self.chain_of_states[i+1])][self.chain_of_actions[i]])\n",
    "\n",
    "    def train(self):\n",
    "        # done = False\n",
    "        \n",
    "        for i in range(self.no_of_episodes):\n",
    "            self.current_time_steps = 0\n",
    "            self.done_or_not = False\n",
    "            while not self.done_or_not:\n",
    "                # self.render()\n",
    "                observation, reward, self.done_or_not, _ = self.step()\n",
    "                if self.done_or_not:\n",
    "                    print(reward)\n",
    "                    \n",
    "                    break\n",
    "            # self.render()\n",
    "            self.compute_q_values()\n",
    "            self.reset()\n",
    "            print('Episode Completed')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# class environment_q_learning:\n",
    "#     def __init__(self,type_of_env:str,gamma_disc: float, epsilon:float,epsilon_decay:float,learning_rate:float,no_of_episodes:int,max_time_steps:int):\n",
    "environment_q_learning_obj = environment_q_learning(type_of_env = 'deterministic',gamma_disc=0.9,epsilon=0.2,epsilon_decay=0.001,learning_rate=0.5,no_of_episodes=15,max_time_steps=10000)\n",
    "environment_q_learning_obj.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n",
      "Episode Completed\n",
      "12\n",
      "Episode Completed\n",
      "14\n",
      "Episode Completed\n",
      "11\n",
      "Episode Completed\n",
      "9\n",
      "Episode Completed\n",
      "14\n",
      "Episode Completed\n",
      "15\n",
      "Episode Completed\n",
      "15\n",
      "Episode Completed\n",
      "15\n",
      "Episode Completed\n",
      "14\n",
      "Episode Completed\n",
      "14\n",
      "Episode Completed\n",
      "12\n",
      "Episode Completed\n",
      "14\n",
      "Episode Completed\n",
      "14\n",
      "Episode Completed\n",
      "12\n",
      "Episode Completed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "for i, j, k in zip(environment_q_learning_obj.chain_of_actions, environment_q_learning_obj.chain_of_states,environment_q_learning_obj.chain_of_rewards):\n",
    "    print(i,j,k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "environment_q_learning_obj.qvalue_table"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{(0, 0): array([0.        , 5.56212698, 0.        , 5.33499033]),\n",
       " (0, 1): array([0.        , 6.16443198, 4.8781654 , 6.1227077 ]),\n",
       " (0, 2): array([0.        , 6.94127372, 5.40977977, 5.44035809]),\n",
       " (0, 3): array([0.        , 6.60872628, 5.82391777, 5.49792064]),\n",
       " (0, 4): array([0.        , 6.23813524, 5.95095606, 0.        ]),\n",
       " (1, 0): array([4.92910944, 6.22587714, 0.        , 6.18989406]),\n",
       " (1, 1): array([5.44723236, 6.90163378, 5.58008997, 6.89179703]),\n",
       " (1, 2): array([5.76079074, 7.7797964 , 6.1526347 , 7.54763291]),\n",
       " (1, 3): array([5.26139707, 8.7349982 , 6.72438745, 6.66409273]),\n",
       " (1, 4): array([5.43623261, 7.76356542, 7.55136867, 0.        ]),\n",
       " (2, 0): array([5.52472509, 6.24032576, 0.        , 6.92757878]),\n",
       " (2, 1): array([6.119614  , 7.27196888, 5.97403895, 7.48482439]),\n",
       " (2, 2): array([6.84101916, 8.71732784, 6.5447856 , 7.98023901]),\n",
       " (2, 3): array([6.54531165, 8.02678322, 7.21817187, 7.36516552]),\n",
       " (2, 4): array([6.74416141, 8.46390368, 8.71187648, 0.        ]),\n",
       " (3, 0): array([6.1691477 , 6.2859018 , 0.        , 7.75019092]),\n",
       " (3, 1): array([6.86668522, 7.2005873 , 6.74343325, 8.9181194 ]),\n",
       " (3, 2): array([7.21069166, 8.04362848, 7.34367523, 7.88531342]),\n",
       " (3, 3): array([8.66063698, 8.93353271, 7.0562981 , 7.7980957 ]),\n",
       " (3, 4): array([6.73838408, 9.6875    , 7.94006653, 0.        ]),\n",
       " (4, 0): array([6.21447591, 0.        , 0.        , 7.14182897]),\n",
       " (4, 1): array([7.73700139, 0.        , 6.15979844, 8.05667855]),\n",
       " (4, 2): array([8.64104348, 0.        , 7.22937209, 8.96072388]),\n",
       " (4, 3): array([7.76752625, 0.        , 7.88716736, 9.98046875]),\n",
       " (4, 4): array([0., 0., 0., 0.])}"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9ec5a702473176f6b5ce6c18e1a25a3b92dc3c0568fd1d66af71c175925cbdff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}